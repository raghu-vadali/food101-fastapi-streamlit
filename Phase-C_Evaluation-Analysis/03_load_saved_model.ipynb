{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2a104c",
   "metadata": {},
   "source": [
    "### 03_load_saved_model.ipynb is a smoke test \n",
    "\n",
    "that verifies the exported model loads correctly, accepts inputs in the expected format, and produces valid outputs.\n",
    "\n",
    "Nothing more, nothing less.\n",
    "\n",
    "SavedModel (.pb) loads without error\n",
    "\n",
    "Input shape and preprocessing are correct\n",
    "\n",
    "Inference runs in production-style (inference-only) mode\n",
    "\n",
    "Output structure and dimensions match expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8319982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 21:47:20.428131: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-16 21:47:23.917914: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0-dev0+selfbuilt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc6f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45dbc6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"/mnt/d/data/food101\")\n",
    "TEST_DIR = DATA_DIR / \"test\"\n",
    "\n",
    "SAVED_MODEL_DIR = pathlib.Path(\n",
    "    \"/mnt/d/04_Food101-EfficientNetV2S-model/models/export/food101_savedmodel\"\n",
    ")\n",
    "\n",
    "# should be the same as used during training\n",
    "IMG_SIZE = (384, 384)\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe44637",
   "metadata": {},
   "source": [
    "\n",
    "Keras 3 treats TensorFlow SavedModel as an inference-only graph, not a full Keras model, so it cannot be loaded directly with load_model().\n",
    "TFSMLayer explicitly wraps the SavedModel and specifies which exported function (serving_default) to call.\n",
    "Wrapping it in Sequential provides a Keras-compatible model interface for evaluation and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b448f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768596446.104664   24512 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5518 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:64:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ tfsm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TFSMLayer</span>)          â”‚ ?                      â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,590,021</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ tfsm_layer (\u001b[38;5;33mTFSMLayer\u001b[0m)          â”‚ ?                      â”‚    \u001b[38;5;34m20,590,021\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,590,021</span> (78.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,590,021\u001b[0m (78.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,436,149</span> (77.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,436,149\u001b[0m (77.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">153,872</span> (601.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m153,872\u001b[0m (601.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import TFSMLayer\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    TFSMLayer(SAVED_MODEL_DIR.as_posix(), \n",
    "              call_endpoint=\"serving_default\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61a794",
   "metadata": {},
   "source": [
    "Since the model was trained using TFDS, the evaluation dataset is reloaded from TFDS (food101 test split) to ensure consistent preprocessing, label ordering, and deterministic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178057ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "(ds_train, ds_val), ds_info = tfds.load(\n",
    "                                        \"food101\",\n",
    "                                        split=[\"train\", \"validation\"],\n",
    "                                        shuffle_files=False,\n",
    "                                        as_supervised=True,\n",
    "                                        with_info=True,\n",
    "                                    )\n",
    "ds_val.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b0bb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='food101',\n",
       "    full_name='food101/2.0.0',\n",
       "    description=\"\"\"\n",
       "    This dataset consists of 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.\n",
       "    \"\"\",\n",
       "    homepage='https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/',\n",
       "    data_dir='/home/raghu/tensorflow_datasets/food101/2.0.0',\n",
       "    file_format=tfrecord,\n",
       "    download_size=4.65 GiB,\n",
       "    dataset_size=4.77 GiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=101),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    nondeterministic_order=False,\n",
       "    splits={\n",
       "        'train': <SplitInfo num_examples=75750, num_shards=32>,\n",
       "        'validation': <SplitInfo num_examples=25250, num_shards=16>,\n",
       "    },\n",
       "    citation=\"\"\"@inproceedings{bossard14,\n",
       "      title = {Food-101 -- Mining Discriminative Components with Random Forests},\n",
       "      author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n",
       "      booktitle = {European Conference on Computer Vision},\n",
       "      year = {2014}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6108c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ds_info.features[\"label\"].names\n",
    "num_classes = ds_info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eec6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (384, 384)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess_fn(image, label):\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.keras.applications.efficientnet_v2.preprocess_input(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f45ec5",
   "metadata": {},
   "source": [
    "ğŸš« No augmentation\n",
    "ğŸš« No shuffle\n",
    "ğŸš« No randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6750c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 384, 384, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = (\n",
    "    ds_val\n",
    "    .map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "test_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3d82b",
   "metadata": {},
   "source": [
    "a TensorFlow SavedModel exposes named output signatures, not a single return tensor.\n",
    "When you load it via TFSMLayer, Keras calls the SavedModelâ€™s serving_default function, which returns a dictionary of outputs (e.g., {\"outputs\": tensor}) instead of a raw tensorâ€”this preserves explicit output names for production and avoids ambiguity when models have multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b16647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 21:47:38.432053: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:396] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2026-01-16 21:47:40.080762: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['output_layer'])\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(32, 384, 384, 3) (32, 101)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "images, labels = next(iter(test_ds))\n",
    "preds = model(images)\n",
    "print(preds.keys())\n",
    "\n",
    "print(type(images))\n",
    "preds_tensor = preds[\"output_layer\"]\n",
    "print(images.shape, preds_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
