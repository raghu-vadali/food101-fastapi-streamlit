{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2a104c",
   "metadata": {},
   "source": [
    "### For evaluation, we need to build the model again(not training)\n",
    "\n",
    "if model is not saved as keras then model should be rebuiltas original and checkpoints should be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc829c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" # Enable legacy Keras behavior ignoring keras-3 as model and training checkpoints are saved in legacy keras format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b52073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 13:16:52.065002: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-15 13:16:55.807528: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a4d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 384\n",
    "NUM_CLASSES = 101\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32087e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768479417.868436   10350 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5518 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:64:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "                                         tf.keras.layers.RandomRotation(0.2),\n",
    "                                         tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2)],                                      \n",
    "                                         # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetV2B0\n",
    "                                        name =\"data_augmentation\")\n",
    "\n",
    "\n",
    "# Preprocessing image\n",
    "def preprocess_fn(image, label):\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = preprocess_input(image)    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "def build_training_dataset(dataset, BUFFER_SIZE, BATCH_SIZE):    \n",
    "    # shuffle images before batching up\n",
    "    dataset_processed = (\n",
    "                            dataset\n",
    "                            .shuffle(BUFFER_SIZE)\n",
    "                            .map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                            .map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "                            .batch(BATCH_SIZE)\n",
    "                            .prefetch(1)\n",
    "                        )\n",
    "    return dataset_processed\n",
    "\n",
    "def build_eval_dataset(dataset, BATCH_SIZE):\n",
    "    return (\n",
    "        dataset\n",
    "        .map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(1)\n",
    "    )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d781fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_food101_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=101, dropout_rate=0.5):\n",
    "\n",
    "    # Setup base model, freezing the base model layers and considering pretrained weights on imagenet\n",
    "    base_model = tf.keras.applications.EfficientNetV2S(include_top=False, \n",
    "                                                       weights=\"imagenet\", \n",
    "                                                       input_shape=input_shape\n",
    "                                                       )  \n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
    " \n",
    "    a1 = base_model(inputs, training=False)   # training = True, only last 20 layers are trainable,  \n",
    "                                          # training = False, BatchNorms and Dropouts are freezed during training  very important\n",
    "\n",
    "    # Hybrid pooling\n",
    "    gap = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(a1)\n",
    "    gmp = tf.keras.layers.GlobalMaxPooling2D(name=\"max_pooling_layer\")(a1)\n",
    "    a2 = tf.keras.layers.Concatenate(name=\"max_avg_pooling\")([gap, gmp])\n",
    "\n",
    "    # Dropout\n",
    "    a3 = tf.keras.layers.Dropout(dropout_rate)(a2)  # add dropout layer and a bit stronger on small data\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\", name=\"output_layer\")(a3)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs), base_model\n",
    "\n",
    "model, base_model = build_food101_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=101, dropout_rate=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ee4d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x725694710e30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"/mnt/d/04_Food101-EfficientNetV2S-model/models/checkpoints/ckpt_model_100p.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b448f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name=\"top1\"),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top5\"),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e611c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/d/04_Food101-EfficientNetV2S-model/models/export/food101_savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/d/04_Food101-EfficientNetV2S-model/models/export/food101_savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "# Export the model in SavedModel format for TensorFlow Serving(keras-3 compatible) not native keras-3 .keras format\n",
    "model.save(\n",
    "    \"/mnt/d/04_Food101-EfficientNetV2S-model/models/export/food101_savedmodel\",\n",
    "    save_format=\"tf\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
